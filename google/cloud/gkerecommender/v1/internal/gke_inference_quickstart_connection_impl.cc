// Copyright 2026 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//      https://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// Generated by the Codegen C++ plugin.
// If you make any local changes, they will be lost.
// source: google/cloud/gkerecommender/v1/gkerecommender.proto

#include "google/cloud/gkerecommender/v1/internal/gke_inference_quickstart_connection_impl.h"
#include "google/cloud/gkerecommender/v1/internal/gke_inference_quickstart_option_defaults.h"
#include "google/cloud/background_threads.h"
#include "google/cloud/common_options.h"
#include "google/cloud/grpc_options.h"
#include "google/cloud/internal/pagination_range.h"
#include "google/cloud/internal/retry_loop.h"
#include <memory>
#include <utility>

namespace google {
namespace cloud {
namespace gkerecommender_v1_internal {
GOOGLE_CLOUD_CPP_INLINE_NAMESPACE_BEGIN
namespace {

std::unique_ptr<gkerecommender_v1::GkeInferenceQuickstartRetryPolicy>
retry_policy(Options const& options) {
  return options
      .get<gkerecommender_v1::GkeInferenceQuickstartRetryPolicyOption>()
      ->clone();
}

std::unique_ptr<BackoffPolicy> backoff_policy(Options const& options) {
  return options
      .get<gkerecommender_v1::GkeInferenceQuickstartBackoffPolicyOption>()
      ->clone();
}

std::unique_ptr<
    gkerecommender_v1::GkeInferenceQuickstartConnectionIdempotencyPolicy>
idempotency_policy(Options const& options) {
  return options
      .get<gkerecommender_v1::
               GkeInferenceQuickstartConnectionIdempotencyPolicyOption>()
      ->clone();
}

}  // namespace

GkeInferenceQuickstartConnectionImpl::GkeInferenceQuickstartConnectionImpl(
    std::unique_ptr<google::cloud::BackgroundThreads> background,
    std::shared_ptr<gkerecommender_v1_internal::GkeInferenceQuickstartStub>
        stub,
    Options options)
    : background_(std::move(background)),
      stub_(std::move(stub)),
      options_(internal::MergeOptions(
          std::move(options), GkeInferenceQuickstartConnection::options())) {}

StreamRange<std::string> GkeInferenceQuickstartConnectionImpl::FetchModels(
    google::cloud::gkerecommender::v1::FetchModelsRequest request) {
  request.clear_page_token();
  auto current = google::cloud::internal::SaveCurrentOptions();
  auto idempotency = idempotency_policy(*current)->FetchModels(request);
  char const* function_name = __func__;
  return google::cloud::internal::MakePaginationRange<StreamRange<std::string>>(
      current, std::move(request),
      [idempotency, function_name, stub = stub_,
       retry = std::shared_ptr<
           gkerecommender_v1::GkeInferenceQuickstartRetryPolicy>(
           retry_policy(*current)),
       backoff = std::shared_ptr<BackoffPolicy>(backoff_policy(*current))](
          Options const& options,
          google::cloud::gkerecommender::v1::FetchModelsRequest const& r) {
        return google::cloud::internal::RetryLoop(
            retry->clone(), backoff->clone(), idempotency,
            [stub](grpc::ClientContext& context, Options const& options,
                   google::cloud::gkerecommender::v1::FetchModelsRequest const&
                       request) {
              return stub->FetchModels(context, options, request);
            },
            options, r, function_name);
      },
      [](google::cloud::gkerecommender::v1::FetchModelsResponse r) {
        std::vector<std::string> result(r.models().size());
        auto& messages = *r.mutable_models();
        std::move(messages.begin(), messages.end(), result.begin());
        return result;
      });
}

StreamRange<std::string>
GkeInferenceQuickstartConnectionImpl::FetchModelServers(
    google::cloud::gkerecommender::v1::FetchModelServersRequest request) {
  request.clear_page_token();
  auto current = google::cloud::internal::SaveCurrentOptions();
  auto idempotency = idempotency_policy(*current)->FetchModelServers(request);
  char const* function_name = __func__;
  return google::cloud::internal::MakePaginationRange<StreamRange<std::string>>(
      current, std::move(request),
      [idempotency, function_name, stub = stub_,
       retry = std::shared_ptr<
           gkerecommender_v1::GkeInferenceQuickstartRetryPolicy>(
           retry_policy(*current)),
       backoff = std::shared_ptr<BackoffPolicy>(backoff_policy(*current))](
          Options const& options,
          google::cloud::gkerecommender::v1::FetchModelServersRequest const&
              r) {
        return google::cloud::internal::RetryLoop(
            retry->clone(), backoff->clone(), idempotency,
            [stub](grpc::ClientContext& context, Options const& options,
                   google::cloud::gkerecommender::v1::
                       FetchModelServersRequest const& request) {
              return stub->FetchModelServers(context, options, request);
            },
            options, r, function_name);
      },
      [](google::cloud::gkerecommender::v1::FetchModelServersResponse r) {
        std::vector<std::string> result(r.model_servers().size());
        auto& messages = *r.mutable_model_servers();
        std::move(messages.begin(), messages.end(), result.begin());
        return result;
      });
}

StreamRange<std::string>
GkeInferenceQuickstartConnectionImpl::FetchModelServerVersions(
    google::cloud::gkerecommender::v1::FetchModelServerVersionsRequest
        request) {
  request.clear_page_token();
  auto current = google::cloud::internal::SaveCurrentOptions();
  auto idempotency =
      idempotency_policy(*current)->FetchModelServerVersions(request);
  char const* function_name = __func__;
  return google::cloud::internal::MakePaginationRange<StreamRange<std::string>>(
      current, std::move(request),
      [idempotency, function_name, stub = stub_,
       retry = std::shared_ptr<
           gkerecommender_v1::GkeInferenceQuickstartRetryPolicy>(
           retry_policy(*current)),
       backoff = std::shared_ptr<BackoffPolicy>(backoff_policy(*current))](
          Options const& options,
          google::cloud::gkerecommender::v1::
              FetchModelServerVersionsRequest const& r) {
        return google::cloud::internal::RetryLoop(
            retry->clone(), backoff->clone(), idempotency,
            [stub](grpc::ClientContext& context, Options const& options,
                   google::cloud::gkerecommender::v1::
                       FetchModelServerVersionsRequest const& request) {
              return stub->FetchModelServerVersions(context, options, request);
            },
            options, r, function_name);
      },
      [](google::cloud::gkerecommender::v1::FetchModelServerVersionsResponse
             r) {
        std::vector<std::string> result(r.model_server_versions().size());
        auto& messages = *r.mutable_model_server_versions();
        std::move(messages.begin(), messages.end(), result.begin());
        return result;
      });
}

StreamRange<google::cloud::gkerecommender::v1::Profile>
GkeInferenceQuickstartConnectionImpl::FetchProfiles(
    google::cloud::gkerecommender::v1::FetchProfilesRequest request) {
  request.clear_page_token();
  auto current = google::cloud::internal::SaveCurrentOptions();
  auto idempotency = idempotency_policy(*current)->FetchProfiles(request);
  char const* function_name = __func__;
  return google::cloud::internal::MakePaginationRange<
      StreamRange<google::cloud::gkerecommender::v1::Profile>>(
      current, std::move(request),
      [idempotency, function_name, stub = stub_,
       retry = std::shared_ptr<
           gkerecommender_v1::GkeInferenceQuickstartRetryPolicy>(
           retry_policy(*current)),
       backoff = std::shared_ptr<BackoffPolicy>(backoff_policy(*current))](
          Options const& options,
          google::cloud::gkerecommender::v1::FetchProfilesRequest const& r) {
        return google::cloud::internal::RetryLoop(
            retry->clone(), backoff->clone(), idempotency,
            [stub](
                grpc::ClientContext& context, Options const& options,
                google::cloud::gkerecommender::v1::FetchProfilesRequest const&
                    request) {
              return stub->FetchProfiles(context, options, request);
            },
            options, r, function_name);
      },
      [](google::cloud::gkerecommender::v1::FetchProfilesResponse r) {
        std::vector<google::cloud::gkerecommender::v1::Profile> result(
            r.profile().size());
        auto& messages = *r.mutable_profile();
        std::move(messages.begin(), messages.end(), result.begin());
        return result;
      });
}

StatusOr<google::cloud::gkerecommender::v1::GenerateOptimizedManifestResponse>
GkeInferenceQuickstartConnectionImpl::GenerateOptimizedManifest(
    google::cloud::gkerecommender::v1::GenerateOptimizedManifestRequest const&
        request) {
  auto current = google::cloud::internal::SaveCurrentOptions();
  return google::cloud::internal::RetryLoop(
      retry_policy(*current), backoff_policy(*current),
      idempotency_policy(*current)->GenerateOptimizedManifest(request),
      [this](grpc::ClientContext& context, Options const& options,
             google::cloud::gkerecommender::v1::
                 GenerateOptimizedManifestRequest const& request) {
        return stub_->GenerateOptimizedManifest(context, options, request);
      },
      *current, request, __func__);
}

StatusOr<google::cloud::gkerecommender::v1::FetchBenchmarkingDataResponse>
GkeInferenceQuickstartConnectionImpl::FetchBenchmarkingData(
    google::cloud::gkerecommender::v1::FetchBenchmarkingDataRequest const&
        request) {
  auto current = google::cloud::internal::SaveCurrentOptions();
  return google::cloud::internal::RetryLoop(
      retry_policy(*current), backoff_policy(*current),
      idempotency_policy(*current)->FetchBenchmarkingData(request),
      [this](
          grpc::ClientContext& context, Options const& options,
          google::cloud::gkerecommender::v1::FetchBenchmarkingDataRequest const&
              request) {
        return stub_->FetchBenchmarkingData(context, options, request);
      },
      *current, request, __func__);
}

GOOGLE_CLOUD_CPP_INLINE_NAMESPACE_END
}  // namespace gkerecommender_v1_internal
}  // namespace cloud
}  // namespace google
